# Playing Atari with Deep Reinforcement Learning ğŸ®ğŸ¤–
**Decision Making Algorithms Seminar Project - TUM**

This project presents a complete implementation and analysis of the groundbreaking 2013 DeepMind paper ["Playing Atari with Deep Reinforcement Learning"](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) by Mnih et al., developed as part of our seminar on Decision Making Algorithms.

The Deep Q-Network (DQN) algorithm revolutionized reinforcement learning by successfully combining deep neural networks with Q-learning, achieving super human-level performance on multiple Atari 2600 games using only raw pixel inputs and game scores.

## ğŸ“– About the Paper

The original DQN paper introduced several key innovations that made deep reinforcement learning feasible:

* **Experience Replay**: Storing and randomly sampling past experiences to break temporal correlations and improve data efficiency
* **End-to-End Learning**: Learning directly from raw pixels to actions without manual feature engineering
* **Human-Level Performance**: First algorithm to achieve human-level control on a wide variety of Atari games

Our implementation faithfully reproduces the architecture and training methodology described in the original paper, providing both educational insights and practical demonstrations of the algorithm's capabilities.

## ğŸ¯ Project Highlights

* **Faithful Implementation**: Complete reproduction of the original DQN architecture and training methodology
* **Multiple Game Support**: Trained and tested on classic Atari games including Pong, Breakout, and Boxing
* **Comprehensive Evaluation**: Extensive testing framework with performance benchmarks and statistical analysis
* **Interactive Demos**: Real-time gameplay visualization with Q-value analysis
* **Educational Tools**: Step-by-step training process with detailed logging and visualization

## ğŸ§  How DQN Works

The DQN algorithm operates through three main components:

### 1. Neural Network Architecture ğŸ—ï¸
* **Convolutional Layers**: Three conv layers (32, 64, 64 filters) for feature extraction from raw pixels
* **Fully Connected Layers**: Two dense layers (512 hidden units) for action-value estimation
* **Input Processing**: 84x84x4 grayscale frames (4 frames stacked for temporal information)

### 2. Training Process ğŸ”„
* **Experience Replay**: Store transitions in replay buffer, sample random minibatches for training
* **Target Network**: Separate target network updated every 10,000 steps for stable Q-value targets
* **Epsilon-Greedy Exploration**: Gradually decay exploration from 100% to 1% over 1M steps

### 3. Environment Preprocessing ğŸ”§
* **Frame Stacking**: Stack 4 consecutive frames to capture motion
* **Grayscale Conversion**: Convert RGB to grayscale for computational efficiency
* **Frame Skipping**: Repeat actions for 4 frames to reduce computational requirements
* **Reward Clipping**: Clip rewards to [-1, +1] for algorithm stability

## ğŸš€ Key Features

* **Original Architecture**: Exact replication of the 2013 DQN network architecture
* **Advanced Training**: Implementation with experience replay, target networks, and epsilon-greedy exploration
* **Multi-Game Support**: Pong, Breakout, Boxing environments with specialized preprocessing
* **Comprehensive Testing**: Evaluation framework with statistical analysis and video recording
* **Interactive Visualization**: Real-time Q-value display and gameplay analysis
* **Model Persistence**: Save and load trained models for evaluation and further training

## ğŸ’» Technology Stack

* **Deep Learning**: PyTorch 2.5.1 for neural network implementation
* **Environment**: Gymnasium with Atari environments (ALE)
* **Preprocessing**: OpenCV for image processing and frame manipulation
* **Visualization**: Matplotlib for training plots and Q-value analysis
* **Development**: Jupyter notebooks for interactive development and analysis

## ğŸ“Š Results

Our DQN implementation successfully demonstrates the key findings from the original paper:

| Game | Original Paper Score | Our Implementation | Human Performance |
|------|---------------------|-------------------|------------------|
| Pong | 18.9 | ~15-20 | 14.6 |
| Breakout | 317.8 | ~250-350 | 30.5 |
| Boxing | 71.8 | ~60-80 | 12.1 |

The agent learns effective strategies including ball tracking in Pong, brick-breaking patterns in Breakout, and defensive/offensive moves in Boxing.

## ğŸ› ï¸ Installation and Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/dqn-atari-seminar
cd dqn-atari-seminar

# Create conda environment
conda create -n dqn-env python=3.9
conda activate dqn-env

# Install dependencies
pip install -r requirements.txt

# Install Atari ROMs (required for legal use)
# Download Atari ROMs from official sources and install
```

## ğŸ® Usage

### Training a New Model

```python
# Train DQN on Pong
python train_dqn.py --game ALE/Pong-v5 --max-frames 10000000

# Train on Breakout with custom parameters
python train_dqn.py --game ALE/Breakout-v5 --batch-size 32 --learning-rate 0.00025
```

### Testing Trained Models

```python
# Test trained Pong model
python test_pong_dqn.py --model models/pong_final.pth --episodes 100

# Interactive demo with Q-value visualization
python test_pong_dqn.py --model models/pong_final.pth --interactive

# Benchmark performance
python test_pong_dqn.py --model models/pong_final.pth --benchmark 100
```

### Jupyter Notebooks

```bash
# Launch Jupyter for interactive training and analysis
jupyter notebook DQN/DeepRL_pong.ipynb
```

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ DQN/
â”‚   â”œâ”€â”€ DeepRL_pong.ipynb          # Pong training notebook
â”‚   â”œâ”€â”€ DeepRL_breakout.ipynb      # Breakout training notebook  
â”‚   â”œâ”€â”€ DeepRL_boxing.ipynb        # Boxing training notebook
â”‚   â”œâ”€â”€ model.py                   # DQN neural network architecture
â”‚   â”œâ”€â”€ utils.py                   # Training utilities and helper functions
â”‚   â”œâ”€â”€ wrappers.py                # Atari environment preprocessing wrappers
â”‚   â”œâ”€â”€ replay_memory.py           # Experience replay buffer implementation
â”‚   â”œâ”€â”€ Param.py                   # Hyperparameters and configuration
â”‚   â”œâ”€â”€ test_pong_dqn.py           # Comprehensive testing and evaluation tools
â”‚   â”œâ”€â”€ models/                    # Saved model checkpoints
â”‚   â”‚   â”œâ”€â”€ pong_final.pth
â”‚   â”‚   â”œâ”€â”€ breakout_final.pth
â”‚   â”‚   â””â”€â”€ boxing_final.pth
â”‚   â””â”€â”€ videos/                    # Recorded gameplay videos
â”‚       â”œâ”€â”€ pong_gameplay.gif
â”‚       â”œâ”€â”€ breakout_gameplay.gif
â”‚       â””â”€â”€ boxing_gameplay.gif
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸ“ Educational Value

This project serves as a comprehensive educational resource for understanding:

* **Deep Reinforcement Learning Fundamentals**: Practical implementation of Q-learning with function approximation
* **Neural Network Design**: Convolutional architectures for processing visual inputs
* **Training Stability**: Techniques for stabilizing deep RL training (experience replay, target networks)
* **Hyperparameter Tuning**: Impact of learning rates, batch sizes, and exploration strategies
* **Performance Evaluation**: Statistical analysis and benchmark methodologies

## ğŸ” Advanced Features

### Q-Value Visualization
Real-time display of Q-values for each possible action, providing insight into the agent's decision-making process.

### Interactive Testing
Step through gameplay frame-by-frame, pause/resume, and analyze specific game states.

### Comprehensive Benchmarking
Statistical evaluation over hundreds of episodes with win/loss analysis and performance metrics.

### Video Recording
Automatic generation of gameplay videos for demonstration and analysis purposes.

## ğŸ“ˆ Performance Analysis

Our implementation includes detailed performance tracking:

* **Training Curves**: Episode rewards, loss values, and exploration decay over time
* **Statistical Analysis**: Mean, standard deviation, and confidence intervals for performance metrics
* **Comparative Studies**: Performance comparison across different games and hyperparameter settings
* **Convergence Analysis**: Training stability and convergence rate evaluation

## ğŸ¤ Contributors

Developed with passion for reinforcement learning by:

* **Luca Bozzetti** - Implementation lead and algorithm optimization
* **Paul VorderbrÃ¼gge** - Environment setup and evaluation framework  
* **Sebastian Rogg** - Testing infrastructure and performance analysis

---

*This project was developed as part of the Decision Making Algorithms seminar, exploring the foundational work that launched the modern era of deep reinforcement learning.*